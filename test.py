# -*- coding: utf-8 -*-
"""test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZvOcrbOGEFCIzY88olcNTgxCmR5dMy4f

# Assumption

* Do not preserve punction and whitespace 
* Work with a single text

# Imports
"""

import pandas as pd
import requests
import numpy as np
import scipy.stats as sps
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()

"""# F0-1: Prepare the source"""

doc_url = 'https://www.gutenberg.org/files/2701/old/moby10b.txt'
moby_text = requests.get(doc_url).text.split('\r\n')
moby_lines_raw = pd.DataFrame(moby_text, columns=['line'])
moby_lines_raw.index.name = 'line_id'

len(moby_lines_raw.index)

moby_start = 318
moby_end = 23238
moby_lines = moby_lines_raw.loc[moby_start:moby_end].copy()
moby_lines.reset_index(drop=True, inplace=True)
moby_lines.index.name = 'line_id'

moby_lines.head(10)

"""# F2: Identify Chapters and Paragraphs

## Chapters

We apply the **milestone method** to identify, label, and group lines.
"""

moby_lines['chap_ms'] = moby_lines.line\
  .str.match(r'^(?:ETYMOLOGY|CHAPTER \d+|Epilog)')

moby_lines.head()

moby_lines['chap_idx'] = None # WHY
moby_lines['chap_idx'] = moby_lines[moby_lines.chap_ms]\
  .apply(lambda x: x.index)
moby_lines.chap_idx = moby_lines.chap_idx.ffill()
moby_lines.chap_idx = moby_lines['chap_idx'].astype('int')

moby_lines.head()

moby_chaps = moby_lines.groupby('chap_idx').line\
  .apply(lambda x: '\n'.join(x[:])).to_frame().reset_index(drop=True)
moby_chaps.index.name = 'chap_id'

moby_chaps.head()

# Optionally removed chapter names here (split and pop off first chunk)

"""## Paragraphs"""

moby_paras = moby_chaps.line.str.split(r'\n\n+', expand=True).stack()\
  .to_frame().reset_index().rename(columns={'level_1':'para_id', 0:'para'})
moby_paras.set_index(['chap_id','para_id'], inplace=True)
moby_paras.para = moby_paras.para.str.replace(r'\n', ' ')

moby_paras.head()

moby_paras.loc[(2,3)].para

"""# F3: Identify Sentences and Tokens

We identify tokens by delimitter parsing. Delimitters are represented by character classes.

## Sentences

Sentence are actually a container.
"""

moby_sent = moby_paras.para.str.split(r'(?:[":;.?!\(\)]|--)', expand=True)\
  .stack()\
  .to_frame().reset_index().rename(columns={'level_2':'sent_id', 0:'sent'})
moby_sent.set_index(['chap_id', 'para_id', 'sent_id'], inplace=True)
moby_sent = moby_sent[~moby_sent.sent.str.match(r'^\s*$')]

# Re-order the sentence IDs -- NEED TO GENERALIZE THIS PATTERN
moby_sent['new_sent_idx'] = moby_sent.groupby(['chap_id','para_id']).cumcount()
moby_sent = moby_sent.reset_index()
moby_sent.rename(columns={'sent_id':'delete_me'}, inplace=True)
moby_sent.rename(columns={'new_sent_idx':'sent_id'}, inplace=True)
moby_sent = moby_sent.set_index(['chap_id','para_id','sent_id'])['sent']\
  .to_frame()

moby_sent.head()

"""## Tokens"""

moby_tokens = moby_sent.sent.str.split(r'\W+', expand=True).stack()\
  .to_frame().reset_index().rename(columns={'level_3':'token_id', 0:'token'})
moby_tokens.set_index(['chap_id', 'para_id', 'sent_id', 'token_id'], inplace=True)

moby_tokens.head()

moby_tokens['norm'] = moby_tokens.token.str.lower()

moby_tokens.head()

"""# Create Vocabulary"""

moby_vocab = pd.DataFrame(moby_tokens.token.str.lower().sort_values().unique(), columns=['term'])
moby_vocab.index.name = 'term_id'

moby_vocab.sample(10)

"""## Get Term ID function"""

def term_id(term):
  try:
    return moby_vocab.query("term == @term").index[0]
  except IndexError as e:
    return None

"""## Stopwords"""

stopwords = requests.get('https://algs4.cs.princeton.edu/35applications/stopwords.txt').text.split('\n')
stopwords[:5]
moby_vocab['sw'] = moby_vocab.term.apply(lambda x: x in stopwords or len(x) < 2 or x.isdigit())

moby_vocab.sample(10)

"""## Replace terms with IDs"""

moby_tokens['term_id'] = moby_tokens.norm.map(moby_vocab.reset_index()\
  .set_index('term').term_id)\
  .fillna(-1).astype('int')

moby_tokens.head()

moby_tokens = moby_tokens[['token','term_id']]

moby_tokens.head()

"""## Remove stopwords"""

moby_tokens['sw'] = moby_tokens.term_id.map(moby_vocab.sw)
moby_tokens_ns = moby_tokens.loc[~moby_tokens.sw, ['term_id']]

moby_tokens_ns.head()

"""## Dispersion Plots of 'ahab' and 'whale'

### Add raw term counts to vocab
"""

moby_vocab['n'] = moby_tokens.groupby('term_id').term_id.count()

moby_vocab.sample(10)

"""### Convert tokens into OHE matrix"""

kahuna = pd.get_dummies(moby_tokens_ns.reset_index()['term_id']).T

"""### Default method"""

def get_term_dplot(term, figsize=(15,.5)):
  print(term)
  kahuna.loc[term_id(term)].plot(figsize=figsize)
  plt.show()

terms = ['ahab','whale', 'starbuck', 'queequeg', 'ishmael', 'white', 'sea', 'ship', 'church', 'death']
for term in terms:
  get_term_dplot(term)

"""### Seaborn method"""

df1 = pd.DataFrame({term:kahuna.loc[term_id(term)] for term in terms})
df1.index.name = 't'

df1.head()

df2 = df1.stack().to_frame().reset_index().rename(columns={'level_1':'term', 0:'n'})

# set size of figure
plt.figure(figsize=(15,5))

# use horizontal stripplot with x marker size of 5
sns.stripplot(y='term', x='t', data=df2[df2.n == 1],
 orient='h', marker="$|$", color='navy', size=10)

# rotate x tick labels
plt.xticks(rotation=15)

# remove borders of plot
#plt.tight_layout()
plt.show()

"""# Entropy and KDE?

Bin the frequencies and calculate entropy and mutual information of terms
Compare to

# Create BOWs and TDMs

## BOW by Para
"""

moby_bow = moby_tokens_ns\
  .groupby(['chap_id','para_id','term_id'])\
  .term_id.count()\
  .to_frame().rename(columns={'term_id':'n'})

moby_bow.head()

"""## BOW by Chap"""

moby_bow_chaps = moby_tokens_ns\
  .groupby(['chap_id','term_id'])\
  .term_id.count()\
  .to_frame().rename(columns={'term_id':'n'})

moby_bow_chaps.head()

"""## BOW by Chunk"""

moby_tokens_ns['ord'] = moby_tokens_ns.reset_index().index + 1

# Divide text evenly in K chunks
K = 100
N = moby_tokens_ns.ord.max()
C = int(round(N / K))
T = C * K

N, K, C, T

moby_tokens_ns['chunk'] = moby_tokens_ns.ord.div(C).round().astype('int')

moby_tokens_ns.head()

# moby_tokens_ns.chunk.value_counts()

moby_bow_chunks = moby_tokens_ns\
  .groupby(['chunk','term_id'])\
  .term_id.count()\
  .to_frame().rename(columns={'term_id':'n'})

# moby_bow_chunks.unstack().fillna(0).sum(1)

"""## DTM by Para"""

moby_dtm = moby_bow.unstack().fillna(0)
moby_dtm.columns = moby_dtm.columns.droplevel()

moby_dtm.head()

"""## DTM by Chap"""

moby_dtm_chaps = moby_bow_chaps.unstack().fillna(0)
moby_dtm_chaps.columns = moby_dtm_chaps.columns.droplevel()

moby_dtm_chaps.head()

"""## DTM by Chunk"""

moby_dtm_chunks = moby_bow_chunks.unstack().fillna(0)
moby_dtm_chunks.columns = moby_dtm_chunks.columns.droplevel()

moby_dtm_chunks.head()

"""## Some graphs"""

WHALE = term_id('whale')
AHAB = term_id('ahab')

WIDE = (15, 5)
THIN = (5, 10)

moby_dtm[WHALE].plot(figsize=WIDE)

moby_dtm[AHAB].plot(figsize=WIDE)

"""# Create TFIDF Matrix

## Get N docs
"""

N = len(moby_dtm.index)

N

"""## TFIDF  by Para"""

moby_dtm_tfidf = moby_dtm.apply(lambda row: row / row.sum(), 1).apply(lambda col: col * np.log(N/col[col > 0].count()))

moby_dtm_tfidf.head()

"""## TFIDF by Chap"""

moby_dtm_tfidf_chaps = moby_dtm_chaps.apply(lambda row: row / row.sum(), 1).apply(lambda col: col * np.log(N/col[col > 0].count()))

# moby_dtm_tfidf_chaps.shape()

moby_dtm_tfidf_chaps.head()

"""## TFIDF by Chunk"""

moby_dtm_tfidf_chunks = moby_dtm_chunks.apply(lambda row: row / row.sum(), 1).apply(lambda col: col * np.log(N/col[col > 0].count()))

moby_dtm_tfidf_chunks.head()

"""# Term Frequency Graphs

## TFIDF by Para
"""

moby_dtm_tfidf.T.loc[AHAB].plot(figsize=WIDE)

moby_dtm_tfidf.T.loc[WHALE].plot(figsize=WIDE)

"""## TFIDF by Chap"""

moby_dtm_tfidf_chaps.T.loc[AHAB].plot(figsize=WIDE)

moby_dtm_tfidf_chaps.T.loc[WHALE].plot(figsize=WIDE)

"""## TFIDF by Chunk"""

moby_dtm_tfidf_chunks.T.loc[AHAB].plot(figsize=WIDE)

moby_dtm_tfidf_chunks.T.loc[WHALE].plot(figsize=WIDE)

"""# Stats"""

import scipy.stats as sps



words_ahab = moby_dtm_tfidf_chunks.T.loc[AHAB]
words_whale = moby_dtm_tfidf_chunks.T.loc[WHALE]

sps.ttest_rel(words_ahab, words_whale)

sps.ttest_ind(words_ahab, words_whale)

